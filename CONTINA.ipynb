{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import DCRNN,causal_model,AGCRN,GWNET,STGCN,MTGNN\n",
    "import tools\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import yaml\n",
    "import os\n",
    "import logging\n",
    "import tools.data_tools\n",
    "import time\n",
    "from itertools import product\n",
    "import csv\n",
    "\n",
    "\n",
    "import tools.train_tools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import random\n",
    "def calculate_coverage(true_values, predicted_means, predicted_lower_bounds, predicted_upper_bounds):\n",
    "    coverage_rates = []\n",
    "\n",
    "    for true_val, lower_bound, upper_bound in zip(true_values, predicted_lower_bounds, predicted_upper_bounds):\n",
    "        # 检查每个真实值是否在预测的上下界内\n",
    "        coverage = np.logical_and(lower_bound <= true_val, true_val <= upper_bound)\n",
    "\n",
    "        coverage_rates.append(coverage)\n",
    "\n",
    "    return coverage_rates\n",
    "def report_result(coverage, upper_bound, lower_bound, true_values, intere_index,alpha=0.1):\n",
    "    \"\"\"\n",
    "    计算覆盖率、区间长度以及 MIS，并返回结果。\n",
    "    \n",
    "    参数:\n",
    "        coverage (array-like): 预测区间覆盖率的布尔值数组。\n",
    "        upper_bound (array-like): 预测区间的上限。\n",
    "        lower_bound (array-like): 预测区间的下限。\n",
    "        true_values (array-like): 实际值。\n",
    "        alpha (float): 显著性水平，默认为 0.05。\n",
    "    \n",
    "    返回:\n",
    "        pd.DataFrame: 包含覆盖率、区间长度和 MIS 的数据框。\n",
    "    \"\"\"\n",
    "    result = dict()\n",
    "    coverage = np.array(coverage)[:,intere_index]\n",
    "    for i in range(4):\n",
    "        # 定义区间范围\n",
    "        if i <= 2:\n",
    "            range_start = i * (24 * 30)\n",
    "            range_end = (i + 1) * (24 * 30)\n",
    "        else:\n",
    "            range_start = i * (24 * 30)\n",
    "            range_end = len(coverage)\n",
    "\n",
    "        # 提取区间对应数据\n",
    "        coverage_segment = coverage[range_start:range_end]\n",
    "        upper_bound_segment = upper_bound[range_start:range_end]\n",
    "        lower_bound_segment = lower_bound[range_start:range_end]\n",
    "        true_values_segment = true_values[range_start:range_end]\n",
    "\n",
    "        # 计算覆盖率和区间长度\n",
    "        result[f'coverage_{i}'] = np.mean(coverage_segment)\n",
    "        result[f'interval_len_{i}'] = np.mean(upper_bound_segment - lower_bound_segment)\n",
    "        result[f'minLC_{i}'] = np.mean(coverage_segment,axis=(0,-1)).min()\n",
    "\n",
    "        # 计算 MIS\n",
    "        interval_length = upper_bound_segment - lower_bound_segment\n",
    "        penalty_below = 2 / alpha * (lower_bound_segment - true_values_segment) * (true_values_segment < lower_bound_segment)\n",
    "        penalty_above = 2 / alpha * (true_values_segment - upper_bound_segment) * (true_values_segment > upper_bound_segment)\n",
    "        mis = np.mean(interval_length + penalty_below + penalty_above)\n",
    "\n",
    "        # result[f'MIS_{i}'] = mis\n",
    "\n",
    "    return pd.DataFrame(result, index=[0])\n",
    "def predict_with_uncertainty(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x = x.to(device)\n",
    "            true_values.append(y.cpu().numpy())\n",
    "            preds = model(x)\n",
    "            predictions.append(preds.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(predictions), np.concatenate(true_values)\n",
    "def compute_conformal_intervals(predictions, residuals, confidence_level=0.9):\n",
    "    # 计算临界值\n",
    "    alpha = 1 - confidence_level\n",
    "    quantile = np.quantile(residuals, confidence_level)\n",
    "    # print(quantile)\n",
    "    # print(predictions.shape)\n",
    "     \n",
    "    # 构造置信区间\n",
    "    lower_bounds = predictions - quantile\n",
    "    upper_bounds = predictions + quantile\n",
    "    \n",
    "    return lower_bounds, upper_bounds\n",
    "def weighted_quantile(values, weights, quantiles, axis=None):\n",
    "    \"\"\"\n",
    "    计算沿指定轴的加权分位数。\n",
    "\n",
    "    :param values: 数据数组 (多维)\n",
    "    :param weights: 权重数组 (与values形状一致，或能广播到values形状)\n",
    "    :param quantiles: 分位数值 (标量或数组，范围 [0, 1])\n",
    "    :param axis: 沿着哪个轴计算分位数 (默认None，对整个数组计算)\n",
    "    :return: 加权分位数值 (结果形状取决于axis和quantiles的输入)\n",
    "    \"\"\"\n",
    "    # 转为 numpy 数组\n",
    "    values = np.array(values)\n",
    "    weights = np.array(weights)\n",
    "    quantiles = np.atleast_1d(quantiles)\n",
    "\n",
    "    if axis is None:\n",
    "        # 展平处理\n",
    "        values = values.ravel()\n",
    "        weights = weights.ravel()\n",
    "        axis = 0\n",
    "\n",
    "    # 移动指定轴到最前面\n",
    "    if axis != 0:\n",
    "        values = np.moveaxis(values, axis, 0)\n",
    "        weights = np.moveaxis(weights, axis, 0)\n",
    "\n",
    "    # 确保 weights 非负且和非零\n",
    "    if np.any(weights < 0):\n",
    "        raise ValueError(\"权重必须为非负值\")\n",
    "    if np.all(weights == 0):\n",
    "        raise ValueError(\"权重不能全为0\")\n",
    "\n",
    "    # 获取新轴形状\n",
    "    shape = values.shape[1:]\n",
    "    quantile_shape = (len(quantiles),) + shape\n",
    "\n",
    "    # 输出初始化\n",
    "    result = np.empty(quantile_shape)\n",
    "\n",
    "    # 遍历指定轴的切片计算分位数\n",
    "    for idx in np.ndindex(*shape):\n",
    "        v_slice = values[(slice(None),) + idx]\n",
    "        w_slice = weights[(slice(None),) + idx]\n",
    "\n",
    "        # 排序值和权重\n",
    "        sorter = np.argsort(v_slice)\n",
    "        v_sorted = v_slice[sorter]\n",
    "        w_sorted = w_slice[sorter]\n",
    "\n",
    "        # 计算累计权重并归一化\n",
    "        cum_weights = np.cumsum(w_sorted)\n",
    "        norm_cum_weights = cum_weights / cum_weights[-1]\n",
    "\n",
    "        # 插值计算分位数\n",
    "        result[(slice(None),) + idx] = np.interp(quantiles, norm_cum_weights, v_sorted)\n",
    "\n",
    "    return result\n",
    "def moving_average(data, window_size):\n",
    "    # 确保输入是一个 numpy 数组\n",
    "    data = np.array(data)\n",
    "\n",
    "    # 如果数据是一维数组，直接进行平滑\n",
    "    if data.ndim == 1:\n",
    "        smoothed_data = np.convolve(data, np.ones(window_size) / window_size, mode='same')\n",
    "    # 如果数据是二维矩阵，逐行进行平滑\n",
    "    elif data.ndim == 2:\n",
    "        smoothed_data = np.zeros_like(data)\n",
    "        for i in range(data.shape[0]):  # 遍历每一行\n",
    "            smoothed_data[i] = np.convolve(data[i], np.ones(window_size) / window_size, mode='same')\n",
    "    else:\n",
    "        raise ValueError(\"Input data must be 1D or 2D.\")\n",
    "\n",
    "    return smoothed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantile_inaxis(x, y, axis):\n",
    "    \"\"\"\n",
    "    计算指定轴上的每个切片的分位数。\n",
    "\n",
    "    参数：\n",
    "        x (numpy.ndarray): 输入多维数组。\n",
    "        y (numpy.ndarray): 分位数值数组，长度需与 x 在指定轴上的大小相等。\n",
    "        axis (int): 指定的轴。\n",
    "\n",
    "    返回：\n",
    "        numpy.ndarray: 计算出的分位数数组。\n",
    "    \"\"\"\n",
    "    if x.shape[axis] != len(y):\n",
    "        raise ValueError(\"The length of y must be equal to the size of the specified axis in x.\")\n",
    "    \n",
    "    # 创建一个索引数组\n",
    "    indices = [slice(None)] * x.ndim\n",
    "    result = []\n",
    "\n",
    "    for i, quantile in enumerate(y):\n",
    "        indices[axis] = i\n",
    "        this_x = x[tuple(indices)]  # 获取指定轴上的切片\n",
    "        this_quantile = np.quantile(this_x, quantile)\n",
    "        result.append(this_quantile)\n",
    "    \n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "def calculate_conformity_scores(true_vals, q_lo, q_hi):\n",
    "    \"\"\"\n",
    "    计算符合性分数 Ei。\n",
    "    \n",
    "    参数:\n",
    "    - true_vals: NumPy 数组，真实值数组。\n",
    "    - q_lo: NumPy 数组，下分位数数组。\n",
    "    - q_hi: NumPy 数组，上分位数数组。\n",
    "    \n",
    "    返回:\n",
    "    - conformity_scores: NumPy 数组，符合性分数数组。\n",
    "    \"\"\"\n",
    "    # 检查输入数组长度是否一致\n",
    "    if not (len(true_vals) == len(q_lo) == len(q_hi)):\n",
    "        print(true_vals.shape, q_lo.shape, q_hi.shape)\n",
    "        raise ValueError(\"输入数组的长度必须一致\")\n",
    "    \n",
    "    # 使用矢量化操作计算符合性分数\n",
    "    conformity_scores = np.where(\n",
    "        true_vals < q_lo,\n",
    "        q_lo - true_vals,\n",
    "        np.where(\n",
    "            true_vals > q_hi,\n",
    "            true_vals - q_hi,\n",
    "            np.maximum(q_lo - true_vals, true_vals - q_hi)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return conformity_scores\n",
    "\n",
    "\n",
    "\n",
    "def CONTINA(model, val_loader, test_loader, device,config,adeptive_lr=False,alpha_ = 0.001  ):\n",
    "    \n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    true_values = []\n",
    "    predictions = []\n",
    "    lower_bounds = []\n",
    "    upper_bounds = []\n",
    "    alpha=0.1\n",
    "    # 第一步：在验证集上计算初始残差集\n",
    "    use_aci=True\n",
    "    dynamic_residuals=True\n",
    "    conformity_scores = []\n",
    "\n",
    "   \n",
    "\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # 模型输出上下分位数\n",
    "        with torch.no_grad():\n",
    "            _,q_lo, q_hi = model(inputs)\n",
    "        \n",
    "        q_lo = q_lo.cpu().numpy()\n",
    "        q_hi = q_hi.cpu().numpy()\n",
    "        true_vals = targets.cpu().numpy()\n",
    "        \n",
    "        # 计算符合性分数 Ei\n",
    "        \n",
    "        conformity_scores.append(calculate_conformity_scores(true_vals,q_lo,q_hi))\n",
    "       \n",
    "    conformity_scores = np.concatenate(conformity_scores)\n",
    "    initial_scores = conformity_scores.copy()  # 保留初始分数集\n",
    "\n",
    "    # 第二步：在测试集上生成预测区间\n",
    "    theta_t=np.array(config['num_nodes']*[0.1])\n",
    "    theta=[]\n",
    "\n",
    " \n",
    "    beta2 = 0.99   # 二阶动量的衰减率\n",
    "    epsilon = 1e-8  # 避免除零的小常数\n",
    "\n",
    "    # 假设 true_val, lower_bound, upper_bound, 和 theta_t 都已经定义\n",
    "    m_t = np.zeros_like(theta_t)  # 一阶动量\n",
    "    v_t = np.zeros_like(theta_t)  # 二阶动量\n",
    "    t = 0  # 时间步\n",
    "   \n",
    "    \n",
    "    with tqdm.tqdm(test_loader, unit=\"batch\", desc=f\"{config['dataset_name']}-{model_name}-{lr}\") as tepoch:\n",
    "        for inputs, targets in tepoch:\n",
    "            theta.append(theta_t.copy())\n",
    "     \n",
    "        \n",
    "            if use_aci:\n",
    "\n",
    "                q_alpha  = get_quantile_inaxis(conformity_scores,1-theta_t,axis=2) #(114)\n",
    "                q_alpha=q_alpha.reshape(1,1,config['num_nodes'],1).repeat(2,-1)\n",
    "\n",
    "            \n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # 模型输出上下分位数\n",
    "            with torch.no_grad():\n",
    "                _,q_lo, q_hi = model(inputs)\n",
    "            \n",
    "            q_lo = q_lo.cpu().numpy()\n",
    "            q_hi = q_hi.cpu().numpy()\n",
    "            true_val = targets.cpu().numpy()\n",
    "            \n",
    "            # 根据 dynamic_residuals 参数选择残差更新模式\n",
    "            \n",
    "            conformity_score=calculate_conformity_scores(true_val,q_lo,q_hi)\n",
    "            \n",
    "        \n",
    "            conformity_scores=np.concatenate([conformity_scores,conformity_score])\n",
    "           \n",
    "            if dynamic_residuals:\n",
    "                conformity_scores = conformity_scores[1:]  # 移除最旧分数\n",
    "        \n",
    "            lower_bound = q_lo - q_alpha\n",
    "            upper_bound = q_hi + q_alpha\n",
    "            prediction = (q_lo + q_hi ) / 2  # 预测均值为中点\n",
    "\n",
    "            # 保存结果\n",
    "            true_values.append(true_val)\n",
    "            predictions.append(prediction)\n",
    "            lower_bounds.append(lower_bound)\n",
    "            upper_bounds.append(upper_bound)\n",
    "\n",
    "            # 如果不使用动态残差，则重置 residuals\n",
    "            if not dynamic_residuals:\n",
    "                conformity_scores = initial_scores.copy()\n",
    "            if use_aci:\n",
    "                for k in range(len(theta_t)):\n",
    "                    \n",
    "                    coverage_errors_this = (true_val[:,:,k,:] < lower_bound[:,:,k,:]) | (true_val[:,:,k,:] > upper_bound[:,:,k,:])\n",
    "                    err_t = np.mean(coverage_errors_this)\n",
    "                    \n",
    "                    # # # 计算梯度（假设是 -err_t，您可以根据实际计算梯度）\n",
    "                    grad_t = alpha-err_t  # 这里假设 err_t 越小，theta_t 越大，所以梯度是负的\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                    t += 1  # 更新时间步\n",
    "\n",
    "                    if adeptive_lr:\n",
    "                \n",
    "                        m_t[k] = grad_t\n",
    "                        v_t[k] = beta2 * v_t[k] + (1 - beta2) * grad_t ** 2\n",
    "                        \n",
    "                        # 偏差修正\n",
    "                        m_hat = m_t[k] / (1)\n",
    "                        v_hat = v_t[k] / (1 - beta2 ** t)\n",
    "                        \n",
    "                        # 更新参数\n",
    "                        theta_t[k] = theta_t[k] + alpha_ * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "                    else:\n",
    "                        theta_t[k] = alpha_* ( alpha-err_t)+theta_t[k]\n",
    "                    # 限制 theta_t 的范围\n",
    "                    theta_t[k] = max(0.01, theta_t[k])\n",
    "                    theta_t[k] = min(0.99, theta_t[k])\n",
    "    \n",
    "    return true_values,predictions,lower_bounds,upper_bounds\n",
    "\n",
    "# # from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9900\\212418416.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(fr'final_models\\{dataset_name}\\{model_name}\\model.pth'))\n",
      "nycbike-STGCN-0.005: 100%|██████████| 2874/2874 [00:35<00:00, 80.02batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   coverage_0  interval_len_0   minLC_0  coverage_1  interval_len_1   minLC_1  \\\n",
      "0    0.899388        0.253525  0.890972    0.900282        0.254412  0.888194   \n",
      "\n",
      "   coverage_2  interval_len_2   minLC_2  coverage_3  interval_len_3   minLC_3  \\\n",
      "0    0.898921        0.277989  0.888194    0.899541        0.343506  0.887255   \n",
      "\n",
      "  dataset_name model_name     lr  \n",
      "0      nycbike      STGCN  0.005  \n"
     ]
    }
   ],
   "source": [
    "sensitive_res=[]\n",
    "for lr in [0.005]:\n",
    "    for dataset_name in ['nycbike']:\n",
    "        for model_name in ['STGCN']:\n",
    "\n",
    "            with open(fr'models\\{dataset_name}_config.yaml', 'r') as f:\n",
    "                        config = yaml.safe_load(f)\n",
    "    \n",
    "        \n",
    "        \n",
    "            config['device']='cuda:0'\n",
    "            train_data, val_data, test_data, scaler,valid_grid=tools.data_tools.get_datasets(config)\n",
    "            adj_mx =  np.load(os.path.join('data\\\\',config['dataset_name'], 'adj_mx.npy'))[valid_grid][:,valid_grid]\n",
    "            config['num_nodes']=len(valid_grid)\n",
    "            \n",
    "            device=torch.device('cuda:0')\n",
    "        \n",
    "            if model_name=='DCRNN':\n",
    "                model = causal_model.CausalModel_quantile_regress(DCRNN.DCRNN,config,adj_mx)\n",
    "            elif model_name=='MTGNN':\n",
    "                model = causal_model.CausalModel_quantile_regress(MTGNN.MTGNN,config,adj_mx)\n",
    "            elif model_name=='GWNET':\n",
    "                model = causal_model.CausalModel_quantile_regress(GWNET.gwnet,config,adj_mx)\n",
    "            elif model_name=='STGCN':\n",
    "                model = causal_model.CausalModel_quantile_regress(STGCN.STGCN,config,adj_mx)\n",
    "           \n",
    "    \n",
    "            \n",
    "            model=model.to(device)\n",
    "            train_loader = DataLoader(train_data, batch_size=config['batch_size'], shuffle=True)\n",
    "            val_loader = DataLoader(val_data, batch_size=config['batch_size'], shuffle=False)\n",
    "            test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "            model.load_state_dict(torch.load(fr'final_models\\{dataset_name}\\{model_name}\\model.pth'))\n",
    "            intere_index=np.where(scaler.inverse_transform(train_data.data).mean(axis=1).mean(axis=1)>2)[0]\n",
    "            true_values, predicted_means, lower_bounds, upper_bounds=CONTINA(model, val_loader, test_loader, device,config,  adeptive_lr=0,alpha_=lr)\n",
    "            true_values=true_values\n",
    "            predicted_means=predicted_means\n",
    "            predicted_lower_bounds=lower_bounds\n",
    "            predicted_upper_bounds=upper_bounds\n",
    "            s = calculate_coverage(true_values, predicted_means, predicted_lower_bounds, predicted_upper_bounds)\n",
    "            np.save(fr'cov\\{dataset_name}-{model_name}.npy',s)\n",
    "            cov_table=report_result(np.array(s)[:,0,0],np.array(predicted_upper_bounds),np.array(predicted_lower_bounds),np.array(true_values)[:,0],intere_index)\n",
    "            cov_table['dataset_name']=dataset_name\n",
    "            cov_table['model_name']=model_name\n",
    "            cov_table['lr']=lr\n",
    "            sensitive_res.append(cov_table)\n",
    "            print(cov_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatglm3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
